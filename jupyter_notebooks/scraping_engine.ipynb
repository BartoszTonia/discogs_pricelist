{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute the file, you need to run scraper in command line\n",
    "\n",
    "```\n",
    "$python3 dprice.py\n",
    "```\n",
    "or\n",
    "```\n",
    "$python3 dprice.py [-u URL] [-l LABEL]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run for best bargains in your country market or make an audit for any seller. Just paste the link.\n",
    "\n",
    "Engine takes any url with Discogs offers. It can be seller inventory...\n",
    "<img src='discogs_view4.png'>\n",
    "\n",
    "\n",
    "..or marketplace page with selected filters like country of shipping, style, format, currency and many more. With pagination, it can check up to 10k results (Discogs limit).\n",
    "\n",
    "<img src='discogs_view.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_page = \"https://www.discogs.com/sell/list?sort=listed%2Cdesc&limit=250&ships_from=Poland&format=Vinyl&style=House&page=1\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "soup = BeautifulSoup(requests.get(url_page).content, 'html.parser')\n",
    "\n",
    "# Each section is written as <tr class=\"shortcut_navigable \"> so we can create a list of listings\n",
    "list_of_listings_soup = soup.select(\"tr.shortcut_navigable\")\n",
    "len(list_of_listings_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `list_of_listings_soup` contains all listings from single search response page. They are grouped into list, so we can iterate over each and create objects with information we want, putting that template on next slices from\n",
    "our soup. Please note that from now, next steps are operate on single item - particular listing offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First object - SearchListObject\n",
    "\n",
    "The first object is lightweight as it is scraped directly from single search response page. \n",
    "\n",
    "It doesn't scrape much information, but just enough to reject unwanted records based on arguments like `condition`, `wants` and `comments`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchListObject:\n",
    "    def __init__( self, listing_select_soup ):\n",
    "        (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First object scrape data marked by red below\n",
    "\n",
    "<img src='discogs_view2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First selection\n",
    "    \n",
    "<a id='wants'>  </a>\n",
    "``wants`` is a number of Discogs users that have current record in their \"wanted\" category. It means they are potential <b> buyers </b> and reflects <b> demand </b>.\n",
    "Engine will look only for records with more 'wants' than given below. It saves time and assures that scraper will look only for wanted records to keep their potential for re-sell.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wants = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`condition` is a Discogs standard and requirement declared by seller for each record. It is common that low quality goes with low prices. As we don't want to see poor quality records, we can reject them at this stage. Below, uncommented condition will NOT be scraped for release and sales statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (\n",
    "    # 'Mint (M)'\n",
    "    # 'Near Mint (NM or M-)'\n",
    "    # 'Very Good Plus (VG+)'\n",
    "    'Very Good (VG)',\n",
    "    'Good Plus (G+)',\n",
    "    'Good (G)',\n",
    "    'Fair (F)', 'Poor (P)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects which doesn't fulfill our requirements are not further investigated, while the others are transformed into second object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second object - DataFrameObject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second object takes arguments from first object..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameObject:\n",
    "    def __init__( self, lst_id, rls_link, seller, have, want, item_title, condition, comment, query ):\n",
    "        (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but also enter the detailed release page for each listing using known `id` to access a load of information like sales statistics, last sold date, ratings and many more. This makes process a lot more time-consuming, but provides complete information about records.\n",
    "\n",
    "<img src='discogs_view3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to write object as a row in temporary csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape engine uses pagination, so each search will be run through next pages until the end of the offers or till page 40 (Discogs limit). When pagination ends <a href='convert.ipynb'> database convert </a> run automatically and temporary file is written as required csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following these steps for a number of searches and records will create a database with good quality records only (and by good quality I mean both physical and subjective perspective). Also this approach significantly shortens scraping time as experience shows than usually lasts only about <b> 25-30% </b> of whole search result - we rejected \"trash\" and compile very clear working list of items for <a href='convert.ipynb'> database convert </a> and <a href='show.ipynb'> further analysis </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
